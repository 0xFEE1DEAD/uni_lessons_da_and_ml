{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba64365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Generator\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import Any, TypedDict\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from faster_whisper import WhisperModel\n",
    "from intervaltree import Interval, IntervalTree\n",
    "from resemblyzer import VoiceEncoder\n",
    "from silero_vad import get_speech_timestamps, load_silero_vad\n",
    "from sklearn.cluster import AgglomerativeClustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb726187",
   "metadata": {},
   "source": [
    "# Диаризация аудио"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d97b5",
   "metadata": {},
   "source": [
    "Задача была сведена к задачи кластеризации. Распространенные модели в виде pyannote, nemo titanet, wespeaker, Gemini DF Resnet используемые во фреймворке sherpa_onnx дают неудовлетворительные результаты для речи на русском языке. Поэтому было принято решение собрать алгоритм самостоятельно используя следующие технологии:\n",
    "* Silero VAD - для решения задачи обнаружения голосовой активности\n",
    "* resemblyzer - для формирования embedding-ов для отрезков аудио\n",
    "* AgglomerativeClustering - из библиотеки sklearn для кластеризации\n",
    "\n",
    "Также для получения объективной оценки качеству решения задачи был разработан бенчмарк, размечен небольшой датасет (около 20 минут аудио). В качестве метрики была выбрана ARI. Были получены следующие результаты:\n",
    "|name|micro_ari|macro_ari|\n",
    "|----|---------|---------|\n",
    "|resemblyzer + silero_vad|0,525|0,507|\n",
    "|pyannote-segmentation-3-0 + 3dspeaker_speech_campplus_sv_zh_en_16k-common_advanced|0,174|0,113|\n",
    "|pyannote-segmentation-3-0 + voxblink2_samresnet34_ft|0,167|0,113|\n",
    "|pyannote-segmentation-3-0 + voxblink2_samresnet100_ft|0,421|0,404|\n",
    "|pyannote-segmentation-3-0 + voxceleb_gemini_dfresnet114_LM|0,276|0,179|\n",
    "|pyannote-segmentation-3-0 + wespeaker_en_voxceleb_resnet293_LM|0,007|0,003|\n",
    "|pyannote-segmentation-3-0 + nemo_en_titanet_large|0,175|0,113|\n",
    "\n",
    "Ознакомится с бэнчмарком можно в папке benchmark/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53318e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment(TypedDict):\n",
    "    start: float\n",
    "    end: float\n",
    "    label: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "323147a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resemblyzer_silero_vad(filepath: Path, n_clusters: int | None = 2) -> Generator[Segment, Any, Any]:\n",
    "    encoder = VoiceEncoder(\"cpu\")\n",
    "    model = load_silero_vad(onnx=True)\n",
    "    wav_numpy, _ = librosa.load(filepath, sr=16000, mono=True)\n",
    "    wav = torch.from_numpy(wav_numpy).float()\n",
    "    speech_timestamps = get_speech_timestamps(\n",
    "        wav,\n",
    "        model,\n",
    "        return_seconds=True,\n",
    "    )\n",
    "\n",
    "    embeddings = []\n",
    "    valid_segments = []\n",
    "\n",
    "    for start, end in map(itemgetter(\"start\", \"end\"), speech_timestamps):\n",
    "        start_sample = int(start * 16000)\n",
    "        end_sample = int(end * 16000)\n",
    "\n",
    "        segment = wav_numpy[start_sample:end_sample]\n",
    "\n",
    "        if len(segment) < 6400:\n",
    "            segment = np.pad(segment, (0, 6400 - len(segment)))\n",
    "\n",
    "        try:\n",
    "            emb = encoder.embed_utterance(segment)\n",
    "            embeddings.append(emb)\n",
    "            valid_segments.append({\"start\": start, \"end\": end})\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "    if embeddings:\n",
    "        embeddings_array = np.array(embeddings)\n",
    "\n",
    "        if len(embeddings_array) > 1:\n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=n_clusters,\n",
    "                distance_threshold=0.90 if n_clusters is None else None,\n",
    "            )\n",
    "            labels = clustering.fit_predict(embeddings_array)\n",
    "\n",
    "            for i, seg in enumerate(valid_segments):\n",
    "                if seg[\"end\"] - seg[\"start\"] > 0.3:\n",
    "                    yield {\n",
    "                        \"start\": seg[\"start\"],\n",
    "                        \"end\": seg[\"end\"],\n",
    "                        \"label\": f\"SPEAKER_{labels[i]:02d}\",\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca2cfa",
   "metadata": {},
   "source": [
    "## Пример работы пайплайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b36aa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cpu in 0.00 seconds.\n"
     ]
    }
   ],
   "source": [
    "segments = list(resemblyzer_silero_vad(Path(\"./audio/3827885e-public_1.wav\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a21788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.2, 'end': 9.8, 'label': 'SPEAKER_00'},\n",
       " {'start': 10.1, 'end': 17.9, 'label': 'SPEAKER_00'},\n",
       " {'start': 19.0, 'end': 20.6, 'label': 'SPEAKER_00'},\n",
       " {'start': 21.5, 'end': 22.4, 'label': 'SPEAKER_00'},\n",
       " {'start': 22.5, 'end': 30.8, 'label': 'SPEAKER_01'},\n",
       " {'start': 31.1, 'end': 37.9, 'label': 'SPEAKER_01'},\n",
       " {'start': 38.1, 'end': 41.4, 'label': 'SPEAKER_01'},\n",
       " {'start': 42.8, 'end': 44.8, 'label': 'SPEAKER_00'},\n",
       " {'start': 47.9, 'end': 51.0, 'label': 'SPEAKER_00'},\n",
       " {'start': 51.3, 'end': 56.4, 'label': 'SPEAKER_01'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444bf695",
   "metadata": {},
   "source": [
    "# Распознавание речи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959f5f1",
   "metadata": {},
   "source": [
    "Эта задача уже очень хорошо решена компанией OpenAI и ее моделью Whisper. Для распознавания используется эта модель. Запуск происходит с помощью библиотеки faster_whisper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a48cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcibe(filepath: Path):\n",
    "    model = WhisperModel(\"medium\", device=\"auto\")\n",
    "\n",
    "    segments, _ = model.transcribe(filepath, word_timestamps=True, vad_filter=True)  # type: ignore  # noqa: PGH003\n",
    "\n",
    "    for segment in segments:\n",
    "        if segment.words is not None:\n",
    "            for word in segment.words:\n",
    "                yield (word.start, word.end, word.word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12de0da2",
   "metadata": {},
   "source": [
    "## Пример работы пайплайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4c97237",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(transcibe(Path(\"./audio/3827885e-public_1.wav\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d103f886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(np.float64(0.0), np.float64(0.24), ' У'),\n",
       " (np.float64(0.24), np.float64(0.42), ' нас,'),\n",
       " (np.float64(0.42), np.float64(0.62), ' к'),\n",
       " (np.float64(0.62), np.float64(0.96), ' сожалению,'),\n",
       " (np.float64(1.18), np.float64(1.8), ' произошли'),\n",
       " (np.float64(1.8), np.float64(2.22), ' некоторые'),\n",
       " (np.float64(2.22), np.float64(3.18), ' неполадки.'),\n",
       " (np.float64(3.26), np.float64(3.68), ' Повторю'),\n",
       " (np.float64(3.68), np.float64(4.14), ' вопрос.'),\n",
       " (np.float64(4.5), np.float64(4.66), ' Вот')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aa5eac",
   "metadata": {},
   "source": [
    "# Сопоставление слов каждому из говорящих"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882f7a11",
   "metadata": {},
   "source": [
    "Для сопоставления слов используется структура данных IntervalTree c оптимальной скоростью вставки и поиском пересекающихся интервалов. (На основе AVL tree все в среднем за log(n)). Сначала записываются интервалы для каждого слова, а затем выполняется поиск слов которые попадают в заданный сегмент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedbf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = IntervalTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c3fb95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    timestamp_to = word[1]\n",
    "    if word[0] == timestamp_to:\n",
    "        timestamp_to += 0.001\n",
    "    tree.add(Interval(word[0], timestamp_to, data=word[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f3d1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPEAKER_00  У нас, к сожалению, произошли некоторые неполадки. Повторю вопрос. Вот мы с тобой ранее обсуждали, что при изучении иностранных языков возникает такой вот\n",
      "SPEAKER_00  вот интересный момент, что ты начинаешь их путать. Вот, можешь показать и рассказать какие-нибудь ситуации, которые у тебя были?\n",
      "SPEAKER_00  Или в целом ощущениях?\n",
      "SPEAKER_00  Быть-было.\n",
      "SPEAKER_01  Ты сначала думаешь о какой-то предложении на русском, потом оно переводится у тебя в клавиатурный английский, ты планируешь сказать это\n",
      "SPEAKER_01  на английском, начинаешь говорить на китайском, оно смешивается с английским, и ты говоришь на двух языках одновременно,\n",
      "SPEAKER_01  но мозг не всегда сразу понимает, что что-то не так.\n",
      "SPEAKER_00  Или ты сформулировал предложение... Это ты имеешь в виду?\n",
      "SPEAKER_00  Это ты имеешь в виду, когда говоришь с иностранцем на английском, да?\n",
      "SPEAKER_01  Когда ты говоришь... Или с преподавателем. Это очень забавно, когда ты говоришь с китайцем,\n"
     ]
    }
   ],
   "source": [
    "for segment in segments[0:10]:\n",
    "    results = tree.overlap(segment[\"start\"], segment[\"end\"])\n",
    "    print(segment[\"label\"], \"\".join(tuple(iv.data for iv in sorted(results))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni-lessons-da-and-ml (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
